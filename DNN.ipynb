{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5650145a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ace01c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, layer_dims, learning_rate=0.001, momentum=0.9, reg_coeff=0.01, dropout_rate=0.1):\n",
    "        super(DNN, self).__init__()\n",
    "\n",
    "        #Store hyperparameter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.reg_coeff = reg_coeff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "\n",
    "        #Network Architecture : 3 layer + BN!!\n",
    "        self.layer1 = nn.Linear(layer_dims[0], layer_dims[1])\n",
    "        self.bn1 = nn.BatchNorm1d(layer_dims[1])\n",
    "        self.layer2 = nn.Linear(layer_dims[1], layer_dims[2])\n",
    "        self.bn2 = nn.BatchNorm1d(layer_dims[2])\n",
    "        self.layer3 = nn.Linear(layer_dims[2], 1)\n",
    "\n",
    "        #dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        #Parameter initialization\n",
    "        nn.init.kaiming_normal_(self.layer1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.layer2.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(self.layer3.weight, nonlinearity='relu')\n",
    "\n",
    "        nn.init.zeros_(self.layer1.bias)\n",
    "        nn.init.zeros_(self.layer2.bias)\n",
    "        nn.init.zeros_(self.layer3.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input image [batch_size, channels, height, width] -> [batch_size, input_dim]\n",
    "        # Pass through the 1st linear layer, apply Batch Normalization, then ReLU and dropout\n",
    "        x = self.layer1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # Pass through the 2nd linear layer, apply Batch Normalization, then ReLU and dropout\n",
    "        x = self.layer2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # Pass through the 3rd linear layer, apply Batch Normalization, then ReLU and dropout\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e22cde",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train() # Set model to training mode (enables dropout, BN)\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = labels.to(device).float().unsqueeze(1) # For BCEWithLogitsLoss, ensure targets are float tensors with shape [batch_size, 1]\n",
    "\n",
    "        optimizer.zero_grad() # Reset gradients\n",
    "        outputs = model(inputs) # Forward pass\n",
    "        loss = criterion(outputs, targets) # Compute loss\n",
    "\n",
    "        loss.backward() #Backpropagation to compute gradients\n",
    "        optimizer.step() # Update model parameters\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        preds = (torch.sigmoid(outputs) >= 0.5).float() # Convert logits to probabilities using sigmoid and classify with threshold 0.5\n",
    "        correct_predictions += (preds == targets).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader.dataset)\n",
    "    accuracy = correct_predictions / len(dataloader.dataset)\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ff381c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval() # Set model to evaluation mode (disables dropout, BN updates)\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient computations for efficiency\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = labels.to(device).float().unsqueeze(1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            preds = (torch.sigmoid(outputs) >= 0.5).float()\n",
    "            correct_predictions += (preds == targets).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader.dataset)\n",
    "    accuracy = correct_predictions / len(dataloader.dataset)\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75ff6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_early_stopping(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=20, patience=5):\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    best_model_state = None\n",
    "    best_train_acc, best_val_acc = 0, 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc * 100:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc * 100:.2f}%\")\n",
    "\n",
    "        # If validation loss improves, save model state and reset the early stop counter\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            early_stopping_counter = 0\n",
    "            best_train_acc = train_acc\n",
    "            best_val_acc = val_acc\n",
    "            print(\"Validation loss improved. Saving best model.\")\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            print(f\"No improvement in validation loss for {early_stopping_counter} epoch(s).\")\n",
    "            if (early_stopping_counter >= patience):\n",
    "                print(\"Early stopping activated. Stopping training.\")\n",
    "                break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    return model, best_train_acc, best_val_acc"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
